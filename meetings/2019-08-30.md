## Topics

- [Agent Based Model](https://en.wikipedia.org/wiki/Agent-based_model)
  An agent-based model (ABM) is a class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness.
  
- [Cellular automaton](https://en.wikipedia.org/wiki/Cellular_automaton) A cellular automaton (pl. 
cellular automata, abbrev. CA) is a discrete model studied in computer science, 
mathematics, physics, complexity science, theoretical biology and microstructure modeling. 
Cellular automata are also called cellular spaces, tessellation automata, homogeneous 
structures, cellular structures, tessellation structures, and iterative arrays.

- [Lambda Calculus](https://en.wikipedia.org/wiki/Lambda_calculus) Lambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. It is a universal model of computation that can be used to simulate any Turing machine. 
It was first introduced by mathematician Alonzo Church in the 1930s as part of his research of the foundations of mathematics.

- [Complex network](https://en.wikipedia.org/wiki/Complex_network) In the context of network theory, 
a complex network is a graph (network) with non-trivial topological features—features that do not occur in simple networks such as lattices or random graphs but often occur in graphs modelling of real systems. The study of complex networks is a young and active area of scientific research[1][2][3][4] (since 2000) inspired largely by the empirical study of real-world networks such as computer networks, technological networks, brain networks and social networks.

- [A New Kind of Science](https://www.wolframscience.com/nks/) 	Preface
1	The Foundations for a New Kind of Science
2	The Crucial Experiment
3	The World of Simple Programs
4	Systems Based on Numbers
5	Two Dimensions and Beyond
6	Starting from Randomness
7	Mechanisms in Programs and Nature
8	Implications for Everyday Systems
9	Fundamental Physics
10	Processes of Perception and Analysis
11	The Notion of Computation
12	The Principle of Computational Equivalence

- [Particle filter](https://en.wikipedia.org/wiki/Particle_filter) Particle filters or Sequential Monte Carlo 
(SMC) methods are a set of Monte Carlo algorithms used to solve filtering problems arising in signal processing 
and Bayesian statistical inference. The filtering problem consists of estimating the internal states in dynamical
systems when partial observations are made, and random perturbations are present in the sensors as well as in 
the dynamical system. The objective is to compute the posterior distributions of the states of some Markov 
process, given some noisy and partial observations. The term "particle filters" was first coined in 1996 by 
Del Moral[1] in reference to mean field interacting particle methods used in fluid mechanics since the beginning
of the 1960s. The terminology "sequential Monte Carlo" was proposed by Liu and Chen in 1998.

- [Rollout algorithms](http://www.mit.edu/~dimitrib/Rollouts_Survey.pdf) by Dimitri P. Bertsekas. 
Rollout is a form of sequential optimization that originated in dynamic programming (DP for short). It may
be viewed as a single iteration of the fundamental method of policy iteration. The starting point is a given
policy (called the base policy), whose performance is evaluated in some way, possibly by simulation. Based
on the evaluation, an improved policy is obtained by one-step lookahead. When the problem is discrete and
deterministic, as will be assumed in this chapter, the method is very simple to implement: the base policy
is just some heuristic, and the rollout policy consists of repeated application of this heuristic. The rollout
policy is guaranteed to improve the performance of the base policy, often very substantially in practice. In
this chapter, rather than using the dynamic programming formalism, the method is explained starting from
first principles

- [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process) A Markov decision process
(MDP) is a discrete time stochastic control process. It provides a mathematical framework for modeling 
decision making in situations where outcomes are partly random and partly under the control of a decision 
maker. MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning.

- [Greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm) A greedy algorithm is an algorithmic 
paradigm that follows the problem solving heuristic of making the locally optimal choice at each stage[1] 
with the intent of finding a global optimum. In many problems, a greedy strategy does not usually produce
an optimal solution, but nonetheless a greedy heuristic 
may yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.

- [Reinforcement learning at Edx](https://www.edx.org/course/reinforcement-learning-explained-10)

- [Applied Category Theory Meeting at UCR](https://johncarlosbaez.wordpress.com/2019/06/16/applied-category-theory-meeting-at-ucr/) 
• Tai-Danae Bradley
• Vin de Silva
• Brendan Fong
• Nina Otter
• Evan Patterson
• Blake Pollard
• Prakash Panangaden
• David Spivak
• Brad Theilman
• Dmitry Vagner
• Zhenghan Wang

- [John Terilla, quantum mechanical approach to NLP](https://www.youtube.com/watch?v=hZfLEX6d-Dk)
John Terilla is a mathematician and CEO of Tunnel Technologies. Tunnel Technologies is building new 
machine learning technology for unsupervised learning targeting natural language processing (NLP).

- [HOMOTOPY PROBABILITY THEORY ON A 
RIEMANNIAN MANIFOLD AND THE EULER EQUATION](https://qcpages.qc.cuny.edu/~jterilla/hpt_and_ns_arxiv.pdf)
GABRIEL C. DRUMMOND-COLE AND JOHN TERILLA. Abstract. Homotopy probability theory is a version of probability theory in
which the vector space of random variables is replaced with a chain complex.
A natural example extends ordinary probability theory on a finite volume
Riemannian manifold M.
I
- [LANGUAGE AS A MATRIX PRODUCT STATE](https://qcpages.qc.cuny.edu/~jterilla/matrixlanguage.pdf)
VASILY PESTUN1
, JOHN TERILLA2
, AND YIANNIS VLASSOPOULOS1
Statistical language modelling, whose aim is to capture the joint probability distribution
of sequences of words, has applications to problems including information retrieval,
speech recognition, artificial intelligence, human-machine interfaces, translation, and natural
language problems that involve incomplete information. Early successes of statistical
language models in industry include next-word prediction and vector embeddings of words
based on colocation with reasonable performance on word similiarity exams. Efforts to
build on early successes encounter difficulties arising from the high-dimensionality of the
data—the number of meaningful texts in a lanugage is exponentially smaller than the number
of texts that a room full of randomly typing monkeys could produce [1]. One approach
to address “the curse of high-dimensionality” is to truncate sequences under consideration
to finite length phrases, or n-grams, and employ a hidden Markov model.

- [Brief notes on category theory](https://www.cs.mcgill.ca/~prakash/Pubs/category_theory_notes.pdf) by 
[Prakash Panangaden](https://www.cs.mcgill.ca/~prakash/)

- [Semantics for Physicists](https://www.youtube.com/watch?v=dkeszesC-io) presentation by Prakash Panangaden. 
 The idea is that things you write are morphisms in some category X, while their meanings are morphisms
 in some other category Y. There’s a functor F : X to Y which sends things you write to their meanings. T
 his functor sends syntax to semantics.
 
